---
title: "Lab 5"
author: "Hannah Lo/Owner(halo-ds) and Yuma Anderson(yumaanderson)"
date: "10/31/2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
```

## Data Preprocess

We are working with a diabetes dataset that contains 19 variables from 403 observations. 
```{r}
data = read.table('diabetes.txt',header=TRUE)
head(data)
```

Missing values in the variable 'frame' were replaced with 'NA'.
```{r}
data$frame[data$frame==''] = NA
data$frame = droplevels(data$frame)
table(data$frame)
```

The variables 'id','bp.2s', and 'bp.2d' were dropped from the dataframe. 
```{r}
diabetes_reduced = select(data,-'id',-'bp.2s',-'bp.2d')
```

Cleaned the dataset by removing any rows with NA values. 
```{r}
index.na=apply(is.na(diabetes_reduced), 1, any)
diabetes_clean = diabetes_reduced[-which(index.na),]
```

To check that all NA values were removed, the number of rows that contain NA values were computed. 
```{r}
index.check=apply(is.na(diabetes_clean), 1, any)
which(index.check)
```

## Exploring and Transforming Data

The variable 'glyhb' is right skewed. We can remedy this by using the logarthmic function on the 'glyhb' variable. Some potential downsides to this approach is that ths transformation is nonlinear; therefore, the insights we gain from looking at it transformed may not translate to the original data. 
```{r}
diabetes_clean %>% ggplot(aes(glyhb))+ geom_histogram(bins=25, fill='pink') +labs(title="Histogram of Glycosolated Hemoglobin",x='Glycosolated Hemoglobin')
```

The variable 'glyhb_star' was created by taking the log and is plotted in the following to show its new distribution: 
```{r}
diabetes_clean$glyhb_star = log(diabetes_clean$glyhb)
diabetes_clean %>% ggplot(aes(glyhb_star))+ geom_histogram(bins=50,fill='red')+labs(title="Histogram of the log of Glycosolated Hemoglobin",x='log(Glycosolated Hemoglobin)')
```

Summary statistics were computed. The following table shows a larger mean glyhb value for a larger frame.
```{r}
diabetes_clean %>% group_by(frame) %>% summarise(mean.glyhb = mean(glyhb_star))
```

The following table shows only a slightly greater mean glyhb for males than females. 
```{r}
diabetes_clean %>% group_by(gender) %>% summarise(mean.glyhb = mean(glyhb_star))
```

The following table shows only a slightly greater mean glyhb for the location Buckingham than Louisa. 
```{r}
diabetes_clean %>% group_by(location) %>% summarise(mean.glyhb = mean(glyhb_star))
```

## Visualizations 
The following is a plot showing the summarized data of `diabetes_clean` grouped by frame and location.
```{r}
diabetes_clean %>% group_by(frame,location) %>% summarise (mean.glyhb_star= mean(glyhb_star))

diabetes_clean %>%
  group_by(frame, location) %>%
  summarise (mean.glyhb_star= mean(glyhb_star)) %>%
  ggplot(aes(x = frame, y = mean.glyhb_star, color=location)) + geom_point() + coord_flip()

```



The relationship between the variables `glyhb_star` and `ratio` seem to be similar across gender. The following plots will have one plot without the gender variable, and one that compares the variables using the `facet_wrap` function separating the plots by gender.
```{r}
ggplot(diabetes_clean,aes(x=glyhb_star,y=ratio,alpha=0.5)) + geom_point() + facet_wrap(~gender) 
```


```{r}
ggplot(diabetes_clean,aes(x=glyhb_star,y=bp.1s,alpha=0.5)) + geom_point() 
ggplot(diabetes_clean,aes(x=glyhb_star,y=bp.1s,alpha=0.5)) + geom_point() + facet_wrap(~gender)
```

```{r}
ggplot(diabetes_clean,aes(x=glyhb_star,y=age,alpha=0.5)) + geom_point()
ggplot(diabetes_clean,aes(x=glyhb_star,y=age,alpha=0.5)) + geom_point() + facet_wrap(~gender) 
```


```{r}
ggplot(diabetes_clean,aes(x=glyhb_star,y=hip,alpha=0.5)) + geom_point()
ggplot(diabetes_clean,aes(x=glyhb_star,y=hip,alpha=0.5)) + geom_point() + facet_wrap(~gender) 
```


```{r}
ggplot(diabetes_clean,aes(x=glyhb_star,y=weight,alpha=0.5)) + geom_point()
ggplot(diabetes_clean,aes(x=glyhb_star,y=weight,alpha=0.5)) + geom_point() + facet_wrap(~gender) 
```



Adding a trendline can improve this visualization and more clearly show the trends between `weight` and `hip` without overplotting.
```{r}
ggplot(diabetes_clean,aes(y=hip,x=waist,alpha=0.5)) + geom_point() + geom_smooth() + facet_wrap(~frame) 
```

Another way to visualize the plot would be to create a density plot to show the pattern of the data.
```{r}
ggplot(diabetes_clean, aes(y=hip, x=waist)) + geom_density_2d() + facet_wrap(~frame)
```


## Messy Data
`gather` and `spread` reshape the data. `spread` copies your data set removing the key and value columns, and replaces them by adding a new column for each unique value of the key column. `gather`, in a way, reverses the `spread` function by collecting a set of column names and replacing them with two columns. These two columns contain the now deleted column names and a column that contains the values of the deleted columns.

No, the functions `spread` and `gather` are not exact complements of each other, because column type information gets lost when using the functions. `gather` discards the original column types and changes the variables into a single vector with one type. By using the `spread` function afterward, we spread the data frame back out and the funtion does not know the original data types of the variables.

## Regression Models

The following regression model was fit with the label glyhb_star and the following features: stabilized glucose, age, waist, cholestrol/hdl ratio and frame. An insight to make from the model is that the F-statistic shows an incredibly small value and tells us that at least one of the variables must be related to glyhb_star. The adjusted R-squared is low, suggesting a linear model was not the right choice. Our exploratory analysis loosely suggested a linear model by the linear relationship seen between glyhb_star and 'frame' in the visualization section. 

```{r}
fit = lm(glyhb_star ~stab.glu + age + waist + ratio+ factor(frame),data=diabetes_clean)
 summary(fit)
```

The following are interpretaions of the estimated regression coefficients. 

******

* The average estimated value for glyhb_star with a large frame and all other variables being zero is '0.8330'.

* The average estimated value for glyhb_star with a medium frame and all other variables being zero is '0.8639'.

* The average estimated value for glyhb_star with a small frame and all other variables being zero is '0.8461'.

* For each increase of one unit of 'stab.glu' the average glyhb_star value with a large frame will increase by ' 0.0035', keeping all other variables constant.

* For each increase of one unit of 'age' the average glyhb_star value with a large frame will increase by '0.0033', keeping all other variables constant.

* For each increase of one unit of 'waist' the average glyhb_star value with a large frame will increase by '0.0047', keeping all other variables constant.

* For each increase of one unit of 'ratio' the average glyhb_star value with a large frame will increase by '0.0219', keeping all other variables constant.

******

Our model estimated fitted values are estimates of the true average of individuals with those observed variables. Using our model, the following is the estimated average glyhb_star value for an individual with these statistics, stab.glu = 90, age = 35, waist = 30, ratio = 5.1, and frame = small.

```{r}
new.values=data.frame( stab.glu =c(90), age=c(35), waist=c(30), ratio=c(5.1), frame=c('small'))

predict(fit,new.values)
```

The difference between inference and prediction is what we plan to get out of our model. For inference, we hope to understand our data we have already seen. For prediction, we hope to predict the outcome for data we have never seen.

The adavantage of using a linear regression model is that it is very simple opposed to a k-NN regression model. This makes it easily interpreted. However, this could be a great disadvantage by over generalizing our data and introducing a large bias. 

## Reflection
We still consider data science to be a field for analyzing and exploring the massive amounts of information we have in todayâ€™s age. We have found that most surprisingly, the field is still very new and changing every day. Most fields in university are set in their base knowledge and remain unchanging, but learning that R is open sourced and ever growing is very interesting. We are most challenged by the sheer amount of concepts to be learned in data science, and the amount of different ways to approach a challenge. Finding the best way to approach an issue is the biggest challenge, and the most enjoyable part of data science.